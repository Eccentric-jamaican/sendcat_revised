# Security: XSS Sanitization for Chat Messages

**Priority**: MEDIUM  
**Cost Impact**: $ (security, not monetary)  
**Estimated Effort**: Small  

## Problem
Chat messages stored from users (and generated by LLM) are not sanitized for XSS attacks. If displayed without escaping, attackers could inject malicious scripts.

## Solution
Sanitize all user inputs and LLM outputs before storage and display.

## Files to Modify
- `package.json` - Add `sanitize-html` dependency
- `convex/mutations/agentJobs.ts` - Sanitize before inserting messages
- Components displaying chat messages - Ensure proper escaping

## Implementation Steps

### Step 1: Add Sanitization Dependency
```bash
bun add sanitize-html
bun add -d @types/sanitize-html
```

### Step 2: Create Sanitization Utility
Create `convex/lib/sanitize.ts`:
```typescript
import sanitizeHtml from "sanitize-html";

/**
 * Sanitize HTML content to prevent XSS attacks
 */
export function sanitizeContent(content: string): string {
  return sanitizeHtml(content, {
    allowedTags: [], // Disallow all HTML tags by default
    allowedAttributes: {},
    disallowedTagsMode: "discard",
  });
}

/**
 * Sanitize for display with limited formatting support
 * (if you want to allow some formatting like bold, links, etc.)
 */
export function sanitizeDisplayContent(content: string): string {
  return sanitizeHtml(content, {
    allowedTags: ["b", "i", "u", "br", "p"],
    allowedAttributes: {},
    disallowedTagsMode: "discard",
  });
}
```

### Step 3: Sanitize User Messages
Modify `convex/mutations/agentJobs.ts`:
```typescript
import { sanitizeContent } from "../lib/sanitize";

export const createAgentJob = mutation({
  args: {
    prompt: v.string(),
    sessionId: v.string(),
    threadId: v.optional(v.id("agentThreads")),
  },
  returns: v.object({ jobId: v.id("agentJobs"), threadId: v.id("agentThreads") }),
  handler: async (ctx, args) => {
    const sanitizedPrompt = sanitizeContent(args.prompt);
    // ... rest of code using sanitizedPrompt ...
    
    await ctx.db.insert("agentMessages", {
      jobId,
      threadId,
      role: "user",
      content: sanitizedPrompt,
      createdAt: now,
    });
    // ...
  },
});
```

### Step 4: Sanitize LLM Responses
Modify `convex/actions/agentRunner.ts`:
```typescript
import { sanitizeContent } from "../lib/sanitize";

// In generateAssistantReply function:
const raw = json.choices?.[0]?.message?.content?.trim() || "Here are a few good options I found.";
const sanitized = sanitizeContent(stripMarkdown(raw));
return sanitized;

// In appendAssistantMessage:
await ctx.runMutation(internal.mutations.agentJobs.appendAssistantMessage, {
  jobId: args.jobId,
  threadId,
  content: sanitizeContent(intent.clarifyingQuestion ?? "What product are you looking for?"),
  createdAt: Date.now(),
});
```

### Step 5: Ensure Frontend Escaping
In any React components displaying messages:
```typescript
// Good - React escapes by default
<div>{message.content}</div>

// Bad - never use this with untrusted content
<div dangerouslySetInnerHTML={{ __html: message.content }} />

// If you need HTML formatting, use the sanitized version
<div dangerouslySetInnerHTML={{ __html: sanitizeDisplayContent(message.content) }} />
```

## Testing Strategy

### Manual Testing
1. Send chat message with XSS payload: `<script>alert('XSS')</script>`
2. Verify script is stripped from stored message
3. Verify script doesn't execute when displayed

### Automated Testing
Create test in `tests/security-xss.test.ts`:
```typescript
import { describe, it, expect } from "bun:test";
import { sanitizeContent } from "../convex/lib/sanitize";

describe("XSS Sanitization", () => {
  it("should strip script tags", () => {
    const input = "<script>alert('XSS')</script>Hello";
    const output = sanitizeContent(input);
    expect(output).toBe("Hello");
    expect(output).not.toContain("<script>");
  });
  
  it("should strip on* event handlers", () => {
    const input = '<div onclick="alert(1)">Click me</div>';
    const output = sanitizeContent(input);
    expect(output).not.toContain("onclick");
  });
  
  it("should strip javascript: URLs", () => {
    const input = '<a href="javascript:alert(1)">Link</a>';
    const output = sanitizeContent(input);
    expect(output).not.toContain("javascript:");
  });
});
```

## Additional Considerations
- **CSP Headers**: Add Content Security Policy headers in `next.config.ts` to block inline scripts
- **Input Validation**: Add length limits to prevent DoS via huge payloads
- **Logging**: Log sanitization events to detect attempted attacks

## CSP Header Configuration
In `next.config.ts`:
```typescript
const nextConfig = {
  async headers() {
    return [
      {
        source: "/(.*)",
        headers: [
          {
            key: "Content-Security-Policy",
            value: "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline';"
          },
        ],
      },
    ];
  },
};
```
